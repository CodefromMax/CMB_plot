{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        p = self.pool(down)\n",
    "\n",
    "        return down, p\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "       x1 = self.up(x1)\n",
    "       x = torch.cat([x1, x2], 1)\n",
    "       return self.conv(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.down_convolution_1 = DownSample(in_channels, 64)\n",
    "        self.down_convolution_2 = DownSample(64, 128)\n",
    "        self.down_convolution_3 = DownSample(128, 256)\n",
    "        self.down_convolution_4 = DownSample(256, 512)\n",
    "\n",
    "        self.bottle_neck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up_convolution_1 = UpSample(1024, 512)\n",
    "        self.up_convolution_2 = UpSample(512, 256)\n",
    "        self.up_convolution_3 = UpSample(256, 128)\n",
    "        self.up_convolution_4 = UpSample(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "       down_1, p1 = self.down_convolution_1(x)\n",
    "       down_2, p2 = self.down_convolution_2(p1)\n",
    "       down_3, p3 = self.down_convolution_3(p2)\n",
    "       down_4, p4 = self.down_convolution_4(p3)\n",
    "\n",
    "       b = self.bottle_neck(p4)\n",
    "\n",
    "       up_1 = self.up_convolution_1(b, down_4)\n",
    "       up_2 = self.up_convolution_2(up_1, down_3)\n",
    "       up_3 = self.up_convolution_3(up_2, down_2)\n",
    "       up_4 = self.up_convolution_4(up_3, down_1)\n",
    "\n",
    "       out = self.out(up_4)\n",
    "       return out\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CMB_Dataset(Dataset):\n",
    "    def __init__(self, root_path, test=False):\n",
    "        self.root_path = root_path\n",
    "        if test:\n",
    "            self.images = sorted([root_path+\"/test_data/\"+i for i in os.listdir(root_path+\"/test_data/\")])\n",
    "            self.masks = sorted([root_path+\"/test_masks/\"+i for i in os.listdir(root_path+\"/test_masks/\")])\n",
    "        else:\n",
    "            self.images = sorted([root_path+\"/train_data/\"+i for i in os.listdir(root_path+\"/train_data/\")])\n",
    "            self.masks = sorted([root_path+\"/train_masks/\"+i for i in os.listdir(root_path+\"/train_masks/\")])\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        mask = Image.open(self.masks[index]).convert(\"L\")\n",
    "\n",
    "        return self.transform(img), self.transform(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)    \n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "DATA_PATH = \"/data\"\n",
    "MODEL_SAVE_PATH = \"/models/unet.pth\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = CMB_Dataset(DATA_PATH)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [0.8, 0.2], generator=generator)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "\n",
    "model = UNet(in_channels=3, num_classes=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    train_running_loss = 0\n",
    "    for idx, img_mask in enumerate(tqdm(train_dataloader)):\n",
    "        img = img_mask[0].float().to(device)\n",
    "        mask = img_mask[1].float().to(device)\n",
    "\n",
    "        y_pred = model(img)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(y_pred, mask)\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_running_loss / (idx + 1)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, img_mask in enumerate(tqdm(val_dataloader)):\n",
    "            img = img_mask[0].float().to(device)\n",
    "            mask = img_mask[1].float().to(device)\n",
    "            \n",
    "            y_pred = model(img)\n",
    "            loss = criterion(y_pred, mask)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "        val_loss = val_running_loss / (idx + 1)\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n",
    "    print(f\"Valid Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class CMB_Dataset(Dataset):\n",
    "    def __init__(self, root_path, test=False):\n",
    "        self.root_path = root_path\n",
    "        if test:\n",
    "            self.images = sorted([os.path.join(root_path, \"test_data\", i) for i in os.listdir(os.path.join(root_path, \"test_data\"))])\n",
    "            self.masks = sorted([os.path.join(root_path, \"test_masks\", i) for i in os.listdir(os.path.join(root_path, \"test_masks\"))])\n",
    "        else:\n",
    "            self.images = sorted([os.path.join(root_path, \"train_data\", i) for i in os.listdir(os.path.join(root_path, \"train_data\"))])\n",
    "            self.masks = sorted([os.path.join(root_path, \"train_masks\", i) for i in os.listdir(os.path.join(root_path, \"train_masks\"))])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ilc_cmb = np.load(self.images[index])\n",
    "        true_cmb = np.load(self.masks[index])\n",
    "\n",
    "        return torch.tensor(ilc_cmb, dtype=torch.float32), torch.tensor(true_cmb, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "# Example usage:\n",
    "DATA_PATH = \"data\"\n",
    "train_dataset = CMB_Dataset(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      " 10%|█         | 1/10 [00:00<00:03,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 1: 0.0049\n",
      "Valid Loss EPOCH 1: 0.0025\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 2: 0.0025\n",
      "Valid Loss EPOCH 2: 0.0010\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      " 30%|███       | 3/10 [00:01<00:02,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 3: 0.0010\n",
      "Valid Loss EPOCH 3: 0.0002\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
      " 40%|████      | 4/10 [00:01<00:02,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 4: 0.0002\n",
      "Valid Loss EPOCH 4: 0.0000\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
      " 50%|█████     | 5/10 [00:01<00:01,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 5: 0.0000\n",
      "Valid Loss EPOCH 5: 0.0002\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      " 60%|██████    | 6/10 [00:02<00:01,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 6: 0.0002\n",
      "Valid Loss EPOCH 6: 0.0004\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      " 70%|███████   | 7/10 [00:02<00:01,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 7: 0.0004\n",
      "Valid Loss EPOCH 7: 0.0004\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      " 80%|████████  | 8/10 [00:03<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 8: 0.0004\n",
      "Valid Loss EPOCH 8: 0.0003\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      " 90%|█████████ | 9/10 [00:03<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 9: 0.0003\n",
      "Valid Loss EPOCH 9: 0.0002\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.06it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 10: 0.0002\n",
      "Valid Loss EPOCH 10: 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "# Define the UNet Model\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        p = self.pool(down)\n",
    "        return down, p\n",
    "\n",
    "# class UpSample(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "#         self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         x1 = self.up(x1)\n",
    "#         x = torch.cat([x1, x2], dim=1)\n",
    "#         return self.conv(x)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# class UpSample(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "#         self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         x1 = self.up(x1)\n",
    "#         # Crop the feature map from the encoder to match the size of the upsampled feature map\n",
    "#         diffY = x2.size()[2] - x1.size()[2]\n",
    "#         diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "#         x2 = x2[:, :, diffY // 2: diffY // 2 + x1.size()[2], diffX // 2: diffX // 2 + x1.size()[3]]\n",
    "\n",
    "#         x = torch.cat([x1, x2], dim=1)\n",
    "#         return self.conv(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Upsample x1\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # Calculate the differences in dimensions\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # Correct x1 dimensions to match x2's dimensions exactly\n",
    "        if diffY > 0:\n",
    "            # Pad if x1 is smaller than x2\n",
    "            x1 = F.pad(x1, (0, 0, diffY // 2, diffY - diffY // 2))\n",
    "        elif diffY < 0:\n",
    "            # Crop if x1 is larger than x2\n",
    "            x1 = x1[:, :, -diffY // 2: x1.size()[2] + diffY // 2, :]\n",
    "\n",
    "        if diffX > 0:\n",
    "            x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, 0, 0))\n",
    "        elif diffX < 0:\n",
    "            x1 = x1[:, :, :, -diffX // 2: x1.size()[3] + diffX // 2]\n",
    "\n",
    "        # Concatenate the feature maps\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.down_convolution_1 = DownSample(in_channels, 64)\n",
    "        self.down_convolution_2 = DownSample(64, 128)\n",
    "        self.down_convolution_3 = DownSample(128, 256)\n",
    "        self.down_convolution_4 = DownSample(256, 512)\n",
    "\n",
    "        self.bottle_neck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up_convolution_1 = UpSample(1024, 512)\n",
    "        self.up_convolution_2 = UpSample(512, 256)\n",
    "        self.up_convolution_3 = UpSample(256, 128)\n",
    "        self.up_convolution_4 = UpSample(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_1, p1 = self.down_convolution_1(x)\n",
    "        down_2, p2 = self.down_convolution_2(p1)\n",
    "        down_3, p3 = self.down_convolution_3(p2)\n",
    "        down_4, p4 = self.down_convolution_4(p3)\n",
    "\n",
    "        b = self.bottle_neck(p4)\n",
    "\n",
    "        up_1 = self.up_convolution_1(b, down_4)\n",
    "        up_2 = self.up_convolution_2(up_1, down_3)\n",
    "        up_3 = self.up_convolution_3(up_2, down_2)\n",
    "        up_4 = self.up_convolution_4(up_3, down_1)\n",
    "\n",
    "        out = self.out(up_4)\n",
    "        \n",
    "        # Crop the output to match the input size\n",
    "        diffY = out.size()[2] - x.size()[2]\n",
    "        diffX = out.size()[3] - x.size()[3]\n",
    "\n",
    "        if diffY != 0 or diffX != 0:\n",
    "            out = out[:, :, diffY // 2: out.size()[2] - diffY // 2, diffX // 2: out.size()[3] - diffX // 2]\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.down_convolution_1 = DownSample(in_channels, 64)\n",
    "#         self.down_convolution_2 = DownSample(64, 128)\n",
    "#         self.down_convolution_3 = DownSample(128, 256)\n",
    "#         self.down_convolution_4 = DownSample(256, 512)\n",
    "\n",
    "#         self.bottle_neck = DoubleConv(512, 1024)\n",
    "\n",
    "#         self.up_convolution_1 = UpSample(1024, 512)\n",
    "#         self.up_convolution_2 = UpSample(512, 256)\n",
    "#         self.up_convolution_3 = UpSample(256, 128)\n",
    "#         self.up_convolution_4 = UpSample(128, 64)\n",
    "        \n",
    "#         self.out = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         down_1, p1 = self.down_convolution_1(x)\n",
    "#         down_2, p2 = self.down_convolution_2(p1)\n",
    "#         down_3, p3 = self.down_convolution_3(p2)\n",
    "#         down_4, p4 = self.down_convolution_4(p3)\n",
    "\n",
    "#         b = self.bottle_neck(p4)\n",
    "\n",
    "#         up_1 = self.up_convolution_1(b, down_4)\n",
    "#         up_2 = self.up_convolution_2(up_1, down_3)\n",
    "#         up_3 = self.up_convolution_3(up_2, down_2)\n",
    "#         up_4 = self.up_convolution_4(up_3, down_1)\n",
    "\n",
    "#         out = self.out(up_4)\n",
    "#         return out\n",
    "\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Define the Dataset for .npy files\n",
    "class CMB_Dataset(Dataset):\n",
    "    def __init__(self, root_path, test=False, target_size=(512, 512)):\n",
    "        self.root_path = root_path\n",
    "        self.target_size = target_size\n",
    "        if test:\n",
    "            self.images = sorted([root_path+\"/test_data/\"+i for i in os.listdir(root_path+\"/test_data/\")])\n",
    "            self.masks = sorted([root_path+\"/test_masks/\"+i for i in os.listdir(root_path+\"/test_masks/\")])\n",
    "        else:\n",
    "            self.images = sorted([root_path+\"/train_data/\"+i for i in os.listdir(root_path+\"/train_data/\")])\n",
    "            self.masks = sorted([root_path+\"/train_masks/\"+i for i in os.listdir(root_path+\"/train_masks/\")])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = np.real(np.load(self.images[index]))\n",
    "        mask = np.real(np.load(self.masks[index]))\n",
    "\n",
    "        # Convert numpy arrays to PIL images for resizing\n",
    "        img_pil = Image.fromarray(img)\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "\n",
    "        # Resize the images\n",
    "        img_resized = F.resize(img_pil, self.target_size)\n",
    "        mask_resized = F.resize(mask_pil, self.target_size)\n",
    "\n",
    "        # Convert back to tensor and add channel dimension\n",
    "        img_tensor = F.to_tensor(img_resized)\n",
    "        mask_tensor = F.to_tensor(mask_resized)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "    # def __init__(self, root_path, test=False):\n",
    "    #     self.root_path = root_path\n",
    "    #     if test:\n",
    "    #         self.images = sorted([root_path+\"/test_data/\"+i for i in os.listdir(root_path+\"/test_data/\")])\n",
    "    #         self.masks = sorted([root_path+\"/test_masks/\"+i for i in os.listdir(root_path+\"/test_masks/\")])\n",
    "    #     else:\n",
    "    #         self.images = sorted([root_path+\"/train_data/\"+i for i in os.listdir(root_path+\"/train_data/\")])\n",
    "    #         self.masks = sorted([root_path+\"/train_masks/\"+i for i in os.listdir(root_path+\"/train_masks/\")])\n",
    "        \n",
    "    #     self.transform = transforms.Compose([\n",
    "    #         transforms.Resize((512, 512)),\n",
    "    #         transforms.ToTensor()])\n",
    "\n",
    "    # def __getitem__(self, index):\n",
    "    #     img = np.load(self.images[index])\n",
    "    #     mask = np.load(self.masks[index])\n",
    "\n",
    "    #     img = img[np.newaxis, :, :]  # Add channel dimension\n",
    "    #     mask = mask[np.newaxis, :, :]  # Add channel dimension\n",
    "\n",
    "    #     return torch.tensor(img, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)    \n",
    "\n",
    "# Training code\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DATA_PATH = \"data\"\n",
    "MODEL_SAVE_PATH = \"models/unet.pth\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = CMB_Dataset(DATA_PATH)\n",
    "\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [0.8, 0.2], generator=generator)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = UNet(in_channels=1, num_classes=1).to(device)  # Adjusted input channels\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()  # MSELoss is more appropriate for residual prediction\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    train_running_loss = 0\n",
    "    for idx, img_mask in enumerate(tqdm(train_dataloader)):\n",
    "        img = img_mask[0].to(device)\n",
    "        mask = img_mask[1].to(device)\n",
    "\n",
    "        y_pred = model(img)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(y_pred, mask)\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_running_loss / (idx + 1)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, img_mask in enumerate(tqdm(val_dataloader)):\n",
    "            img = img_mask[0].to(device)\n",
    "            mask = img_mask[1].to(device)\n",
    "            \n",
    "            y_pred = model(img)\n",
    "            loss = criterion(y_pred, mask)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "        val_loss = val_running_loss / (idx + 1)\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n",
    "    print(f\"Valid Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model(model_path, device='cpu'):\n",
    "    model = UNet(in_channels=1, num_classes=1)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def infer_single_image(model, image_path, device='cpu', target_size=(512, 512)):\n",
    "    # Load and preprocess the image\n",
    "    img = np.load(image_path)\n",
    "    \n",
    "    # If the image is complex, convert to real format\n",
    "    if np.iscomplexobj(img):\n",
    "        img = np.abs(img)\n",
    "    \n",
    "    img = Image.fromarray(img.astype(np.float32))\n",
    "    img_resized = F.resize(img, target_size)\n",
    "    img_tensor = F.to_tensor(img_resized).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "    \n",
    "    # Convert the prediction to a numpy array and remove the batch dimension\n",
    "    prediction_np = prediction.squeeze().cpu().numpy()\n",
    "    \n",
    "    return prediction_np\n",
    "\n",
    "def infer_batch(model, data_loader, device='cpu'):\n",
    "    predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img_batch in data_loader:\n",
    "            img_batch = img_batch[0].to(device)  # Assume the dataloader returns (img, mask)\n",
    "            \n",
    "            # Perform inference\n",
    "            preds = model(img_batch)\n",
    "            \n",
    "            # Convert the predictions to numpy arrays and append to the list\n",
    "            for pred in preds:\n",
    "                predictions.append(pred.squeeze().cpu().numpy())\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = load_model(MODEL_SAVE_PATH, device=device)\n",
    "\n",
    "predicted_CMB = infer_single_image(model, \"data/train_data/ILC_MW_Map_R0000_0.npy\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "import s2fft\n",
    "def mw_alm_2_hp_alm(MW_alm, lmax):\n",
    "    '''MW_alm: 2D array of shape (Lmax, 2*Lmax-1) (MW sampling, McEwen & Wiaux)\n",
    "    '''\n",
    "    # Initialize the 1D hp_alm array with the appropriate size\n",
    "    hp_alm = np.zeros(hp.Alm.getsize(lmax), dtype=np.complex128)\n",
    "        \n",
    "    for l in range(lmax + 1):\n",
    "        for m in range(-l, l + 1):\n",
    "            index = hp.Alm.getidx(lmax, l, abs(m))\n",
    "            if m < 0:\n",
    "                hp_alm[index] = (-1)**m * np.conj(MW_alm[l, lmax + m])\n",
    "            else:\n",
    "                hp_alm[index] = MW_alm[l, lmax + m]\n",
    "\n",
    "    return hp_alm\n",
    "\n",
    "def hp_alm_2_mw_alm(hp_alm, L_max):\n",
    "    MW_alm = np.zeros((L_max, 2 * L_max - 1), dtype=np.complex128)\n",
    "\n",
    "    for l in range(L_max):\n",
    "        for m in range(-l, l + 1):\n",
    "            index = hp.Alm.getidx(L_max - 1, l, abs(m))\n",
    "            if m < 0:\n",
    "                MW_alm[l, L_max + m - 1] = (-1) ** m * np.conj(hp_alm[index])\n",
    "            else:\n",
    "                MW_alm[l, L_max + m - 1] = hp_alm[index]\n",
    "\n",
    "    return MW_alm\n",
    "\n",
    "def visualize_wavelet_coefficient_map(MW_Pix_Map, title, Frequency, min=None, max=None):\n",
    "    \"\"\"\n",
    "    Processes a wavelet coefficient map and visualizes it using HEALPix mollview.\n",
    "    Parameters:\n",
    "        original_70: numpy array representing the wavelet coefficient map.\n",
    "    Returns:\n",
    "        Displays a mollview map.\n",
    "    \"\"\"\n",
    "    # import healpy as hp  # Make sure to import healpy\n",
    "    # from s2fft import forward  # Assuming s2fft.forward is imported\n",
    "    if MW_Pix_Map.shape[0] != 1:\n",
    "        L_max = MW_Pix_Map.shape[0]\n",
    "    else:\n",
    "        L_max = MW_Pix_Map.shape[1]\n",
    "    original_map_alm = s2fft.forward(MW_Pix_Map, L=L_max)\n",
    "    # print(\"Original map alm shape:\", original_map_alm.shape)\n",
    "    \n",
    "    original_map_hp_alm = mw_alm_2_hp_alm(original_map_alm, L_max - 1)\n",
    "    original_hp_map = hp.alm2map(original_map_hp_alm, nside=(L_max - 1)//2)\n",
    "\n",
    "    hp.mollview(\n",
    "        # original_hp_map * 1e5,\n",
    "        original_hp_map,\n",
    "        coord=[\"G\"],\n",
    "        title=title+Frequency,\n",
    "        # unit=r\"$1e5$K\",\n",
    "        # min=min, max=max,  # Uncomment and adjust these as necessary for better visualization contrast\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1,512,512) into shape (1,512,1023)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2780626/3764592470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_wavelet_coefficient_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_CMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Predicted CMB Map \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2780626/708235720.py\u001b[0m in \u001b[0;36mvisualize_wavelet_coefficient_map\u001b[0;34m(MW_Pix_Map, title, Frequency, min, max)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mL_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMW_Pix_Map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0moriginal_map_alm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2fft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMW_Pix_Map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# print(\"Original map alm shape:\", original_map_alm.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/s2fft/transforms/spherical.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(f, L, spin, nside, sampling, method, reality, precomps, spmd, L_lower, iter, _ssht_backend)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mforward_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecomps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"jax\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         return forward_jax(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/s2fft/transforms/spherical.py\u001b[0m in \u001b[0;36mforward_numpy\u001b[0;34m(f, L, spin, nside, sampling, reality, precomps, L_lower)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;31m# Resample mw onto mwss and double resolution of both\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmw_to_mwss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"mw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mwss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0msampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mwss\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/s2fft/utils/resampling.py\u001b[0m in \u001b[0;36mmw_to_mwss\u001b[0;34m(f_mw, L, spin)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf_mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         return np.squeeze(\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mmw_to_mwss_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmw_to_mwss_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         )\n\u001b[1;32m    338\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/s2fft/utils/resampling.py\u001b[0m in \u001b[0;36mmw_to_mwss_theta\u001b[0;34m(f_mw, L, spin)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0msampling\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \"\"\"\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mf_mw_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperiodic_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0mfmp_mwss_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_mw_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/s2fft/utils/resampling.py\u001b[0m in \u001b[0;36mperiodic_extension\u001b[0;34m(f, L, spin, sampling)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mf_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntheta_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mf_ext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mntheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnphi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mntheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnphi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mf_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfftshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1,512,512) into shape (1,512,1023)"
     ]
    }
   ],
   "source": [
    "visualize_wavelet_coefficient_map(predicted_CMB, \"Predicted CMB Map \", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 33 but got size 32 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2780626/3071822239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2780626/1785652247.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottle_neck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mup_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_convolution_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mup_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_convolution_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mup_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_convolution_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2780626/1785652247.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Concatenate the feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 33 but got size 32 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Example with a non-square input shape\n",
    "\n",
    "input_shape = (256, 511)  # Height x Width\n",
    "\n",
    "model = UNet(in_channels=1, num_classes=1)\n",
    "\n",
    "# Simulate a batch of 1 input image with the shape 1x1x256x511 (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(1, 1, input_shape[0], input_shape[1])\n",
    "\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
